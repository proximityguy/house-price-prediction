{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0067837-3d45-4b31-bd8a-ba8b42044c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0    -122.23     37.88                41.0        880.0           129.0   \n",
      "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
      "2    -122.24     37.85                52.0       1467.0           190.0   \n",
      "3    -122.25     37.85                52.0       1274.0           235.0   \n",
      "4    -122.25     37.85                52.0       1627.0           280.0   \n",
      "\n",
      "   population  households  median_income  median_house_value ocean_proximity  \n",
      "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
      "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
      "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
      "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
      "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# loaded the csv file into a variable\n",
    "housing = pd.read_csv(\"../data/housing.csv\")\n",
    "# to print the first 5 rows of the dataset\n",
    "print(housing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe76921-9896-4126-94b8-e63d5a224a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the dataypes and other metadata of the dataset\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799f05c-1a5c-45a9-bd64-c32895ca820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocean_proximity values are only text objects and also seems repetitive, so we'll try to categorize them\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5349997-41cc-4c2b-8ce1-4d47c4d4b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll use describe() method to summerize the numerical attributes\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045675db-8d57-4656-beb0-11dddaa063e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a histogram out of the dataset using matplotlib's hist() method\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins = 50, figsize = (20, 15))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a09fd-d65e-4120-a942-9fc4639aebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test set\n",
    "import numpy as np\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc5d11-6904-4dc6-a842-b45d78e45bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test set\n",
    "train_set, test_set = split_train_test(housing, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124644b-1529-4983-a492-2fc911e8ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of train_set\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2cdc28-474b-4782-8f5b-182271fcd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of test_set\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7680c91-956a-485f-993d-388e525ac16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution to use each instance's identifier to decide whether or not it should go \n",
    "# in the test set (assuming instances have a unique and immutable identifier)\n",
    "\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1ec74-bada-40dc-a540-78ddd8fae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index()   #adds an 'index' column\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd043041-431b-45fa-b539-a3656943b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dcd31-b380-4144-a2a9-ae0190eb6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins = [0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels = [1, 2, 3, 4, 5])\n",
    "\n",
    "# histogram of income categories\n",
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5896a8-2577-4fcc-bab9-2d2fbdc00645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling based on income category\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7575b5-a3af-451c-86d0-e62ef24098e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling\n",
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115521e-54ae-433c-9ced-45a3b413152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will remove the income_cat attribute so the data is back to its original state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12178b1e-ff21-4847-bcf0-5e40d0834b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy so that we can play without harming the training set\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacd83f-e499-43e8-b109-907e7c2219eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have latitiude and longitude, so create a scatterplot of all districts to visualize data\n",
    "housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49377f6b-e86a-4d8c-be6e-68cf5a65b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or may use the alpha option ot make it much easier ot visualize the places where is a high density of data points\n",
    "housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbe2c1-cee8-41a5-a46d-547dc0b28bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will create a pre-defined color map (option cmap) called jet\n",
    "# radius of circles -> population (option s)\n",
    "# color -> price (option c)\n",
    "# colors -> blue (low value) to red (high value)\n",
    "\n",
    "housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.4,\n",
    "             s = housing[\"population\"]/100, label = \"population\", figsize = (10, 7),\n",
    "             c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar = True\n",
    "            )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75735a0-6295-4aa8-b089-9a44447d5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the standard correlation cofficient b/w every pair of attributes using the corr() method\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c5b4a-d7e3-4b69-a254-f0b6e50e87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to check for correlation b/w attributes is to use panda's scatter_matrix function\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_values\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize = (12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8ded2-e87e-464b-a092-93b72604b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation scatterplot of median income vs median house value\n",
    "housing.plot(kind = \"scatter\", x = \"median_income\", y = \"median_house_value\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827e3eb-8f47-4b99-b5ae-0d33ceaa38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data for machine learning algorithm\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_per-room\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"population_per_households\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088181f-9e33-4f07-b8de-a211aacbc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation matrix again\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c67725-2d0b-4059-a419-cff9bda080ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the data so that it does not affect stat_train_set\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis = 1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af952895-c32e-4612-8321-fd1d1956aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "# total_bedrooms attribute have some missing data, so we have 3 options\n",
    "# 1. get rid of the corresponding districts\n",
    "# 2. get rid of the whole attribute\n",
    "# 3. set the values to some value (zero, the mean, the median, etc.)\n",
    "\n",
    "# housing.dropna(subset = [\"total_bedrooms\"])  #option 1\n",
    "housing.drop(\"total_bedrooms\", axis = 1)  #option 2\n",
    "# median = housing[\"total_bedrooms\"].median()  #option 3\n",
    "# housing[\"total_bedrooms\"].fillna(median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918f9da-59af-4151-a18a-5dc134dc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when going with option 3, we need to create a SimpleImputer instance,\n",
    "# specifying that you want to replace each attribute's missing values with the median of that attribute\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "\n",
    "# since median can only be computed on  numerical values,\n",
    "# we need to create a copy of the data without the text attribute ocean_proximity\n",
    "\n",
    "housing_num  = housing.drop(\"ocean_proximity\", axis = 1)\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# imputer will simply calculate the median of aeach attribute and store the result in its statistics_instance variable\n",
    "# right now only total_bedrooms attribute have missing values, but for future need after the system goes live, it is safer to impute to all the numerical attributes\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759e9dc-2e6f-4bc3-881a-f1e0a083661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8292d0-a0c0-49fe-9bac-5a3da3d1250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use this 'trained' imputer to transform the training set by replacing missing values by the learned medians\n",
    "\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "# the result is a plain NumPy array containing thr transformed features\n",
    "# if you want to put it back into a Pandas DataFrame, so run the below code - \n",
    "\n",
    "# housing_tr = pd.DataFrame(X ,columns = housing_num.colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0314cc43-c34a-4eba-b651-d16b9dd696bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# we left out the categorical attribute ocean_proximity becuz it is a text attribute and we can not compute its median\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m housing_cat = \u001b[43mhousing\u001b[49m[[\u001b[33m\"\u001b[39m\u001b[33mocean_proximity\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m      3\u001b[39m housing_cat.head(\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'housing' is not defined"
     ]
    }
   ],
   "source": [
    "# we left out the categorical attribute ocean_proximity becuz it is a text attribute and we can not compute its median\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3a6fbc-05b7-4176-9490-38d79333c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most ML algorithms prefer to work with numbers anyway\n",
    "# so lets convert these categories from text to numbers\n",
    "# we'll use Scikit-Learn's OrdinalÄ“ncoder class\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365b52d-0c30-4d38-822b-3d296a99237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted text attribute to numbers\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f252f-a215-4c51-b3c8-6efdf6165940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get the list of categories using the categories_instance variable\n",
    "# it is a list containing 1-D array of categories for each categorical attribute\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b7885-27d7-4bad-840a-9f4f9f3eaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values\n",
    "# like 0 and 4 are more similar than 0 and 1\n",
    "# solution -> one-hot encoding means to create onen binary attribute per category\n",
    "# new attributes are called 'dummy' attributes\n",
    "# Scikit-Learn provides OneHotEncoder class to convert categorical values into one-hot vectors\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot\n",
    "\n",
    "# notice that the output is a SciPy sparse matrix, instead of a NumPy array \n",
    "# that is very useful when you have categorical attributes with thousands of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d6935-3587-46b6-a683-7f70b88bc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after one-hot encoding we get a matrix with thousands of categories, and matrix is full of zeroes except for single 1 per row\n",
    "# using tons of memory mostly tp store zeros would be very wasteful, so instead a sparse matrix only store the location of the non-zero elements\n",
    "# you can use 2-D array, but if you really want to convert it to a (dense) NumPy array, just call the toarray() method\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f051960-acdc-49dd-8780-3ac7a70fe985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once again you can get the list of categories using the encoder's categories_ instance variable\n",
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4240fb0-81fb-4a8e-95e8-ae724db963c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "room_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def _init_(self, add_bedrooms_per_room = True):  #no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per__room\n",
    "    def fit(self, X, y = None):\n",
    "        return self   #nothing else to do\n",
    "    def transform(self, X, y = None):\n",
    "        rooms_per_househol = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ixx]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[x, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_perhousehold]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room = False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019d59a-0149-4643-8391-7f988e9b2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for the numerical attributes using Pipline class\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = \"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0116d6f8-1180-4298-8260-5630d5cb96cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'housing_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create single transformer which is able to handle all columns, applying the appropriate transformations to each column\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# we'll use the ColumnTransformer apply to all the transformations to the housing data\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m num_attribs = \u001b[38;5;28mlist\u001b[39m(\u001b[43mhousing_num\u001b[49m)\n\u001b[32m      7\u001b[39m cat_attribs = [\u001b[33m\"\u001b[39m\u001b[33mocean_proximity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m full_pipeline = ColumnTransformer([\n\u001b[32m     10\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m\"\u001b[39m, num_pipeline, num_attribs),\n\u001b[32m     11\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m\"\u001b[39m, OneHotEncoder(), cat_attribs),\n\u001b[32m     12\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'housing_num' is not defined"
     ]
    }
   ],
   "source": [
    "# create single transformer which is able to handle all columns, applying the appropriate transformations to each column\n",
    "# we'll use the ColumnTransformer apply to all the transformations to the housing data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957cf14-6f11-4b50-8656-783bd8b79aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and train a model\n",
    "# first train a liner regression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c2fc2-fd94-42f9-9a84-99276fc011c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working linear regression model\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions: \", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels: \", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efba94-fe9e-400c-90d5-d9a371f9bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring the regresion model's RMSE on the whole training set using SciKit-Learn's mean_squared_error function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_sqaured_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88350b94-e995-4839-9109-bc218fbadb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a DecisionTreeRegressor, a poweful model, capable of finding complex nonlinear relationship in the data\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labesl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360908ad-1508-4ebb-a24f-7eaf2e4df5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the trained training set\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tre-rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30829e-7b95-4da1-91c2-700cdd48cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better evaluation using Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9d4bd-5812-4f2c-b090-c5d735664c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of cross-validations\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores),\n",
    "    print(\"Mean: \", scores.mean()),\n",
    "    print(\"Standard deviation: \", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c7ce4-a077-4624-95be-2107147f8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the same scores for the linear regression model just to be sure\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, hosuing_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfcd2c-8237-484a-a37d-281670170c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the RandomForestRegressor model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "[...]\n",
    "forest_rmse\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e989ca7-dc04-40d2-828a-add3e5faabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should save every model we experiment with, so that we can save both the hyperparameters and the trained parameters,\n",
    "# as well as the cross--validation scores and perhaps the actual predictions as well\n",
    "# this will allow you to easily compare scores across model types, and compare the types of errors they make\n",
    "# you can easily save SciKit-Learn models by using Python's pickle module,\n",
    "# or using sklearn.externs.joblib, which is more efficiet at serializing large NumPy\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(my_model, \"my_model.pkl\")\n",
    "\n",
    "# and later\n",
    "my_model_loaded = joblib.load(\"my_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd97aff-c33a-4b6c-98b6-ad78940255ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning\n",
    "# grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "gris_search = GridSearchCV(forest_reg, param_grid, cv = 5,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           return_train_score = True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06dde6-bef8-4a52-a5b0-c58f77277600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best combination of parameters\n",
    "grid_search.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdc453-7960-4622-a3d4-e2b9f958b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best estimators directly\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcddbb16-502e-48b1-9529-013d1ffc0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation scores are\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14048d-60be-4166-958d-b297f18830d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the best model and their errors\n",
    "features_importances = grid_search.best_estimators_.feature_imporatnces_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5eeb45-743f-43a9-888d-c9b50549ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the importance scores next to their corresponding attribute names\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_rooms\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(features_importances, attributes), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233800f-347f-4706-a8f5-0bbf77ed9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your system on the test set\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "x_test = strat_test_set.drop(\"median_house_value\", axis = 1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean.squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)  #evalutes to 47,730.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1928c-0c60-46c3-ad3b-82453006a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find how precise this estimte is, compute a 95% confidence interval for the generalization error using scipy.stats.t.interval()\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95\n",
    "squared_error = (final_predictions - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                         loc = squared_errors.mean(),\n",
    "                         scale = stats.sem(squared_errors)\n",
    "                        )\n",
    "       )\n",
    "\n",
    "# -------- launch, monitor and maintain your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b5e4b-9810-4369-8037-38ec4acfea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
